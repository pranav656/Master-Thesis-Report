\chapter{Literature Survey and Trade-off Analysis}
\label{chp:LitSurvey}
In this chapter, a state-of-the art study will be presented that could assist in design of the lensless imager with specifications mentioned in the previous chapter.
\section{Camera Computational Pipeline}
In order to design a lensless imaging system, we must first look at the computational imaging pipeline of existing cameras. Since the lensless camera basically uses computation to reconstruct images, it is important to understand the computational pipeline of existing camera systems and make necessary modification in the design of the existing pipeline to suit the system. The computational imaging pipeline of existing camera systems is shown figure \ref{fig:CompPipeline}\cite{CompPipeline}.
\begin{figure}[htb]
% most GNUplot figures need to be rotated, width should be the same throughout the complete document, and no extension is needed
\includegraphics[width=\textwidth]{pics/CompPipeline}
\caption{Computational Pipeline of Existing cameras}
\label{fig:CompPipeline}
\end{figure}
As shown in figure \ref{fig:CompPipeline}, there are five main components that can be controlled computationally in existing systems. Illumination of the scene can be controlled to produce an enhanced picture. Optics could be controlled to limit the amount of light entering the scene and thereby controlling the image produced on the sensor. The sensor can also computationally modify the date it receives to de-noise, adjust the blackness/white in an image. Post-processing can also be done on the image produced by the sensor to improve the image produced by the sensor. Finally, a display can also be modified computationally to produce certain effects on the user. And of course, the user can control any of these components to produce the effect he desires. But in the case of the lensless imaging system, we would be modifying the optics and the processing components of the pipeline to reduce the size of the camera.The components to be modified are are darkened in figure \ref{fig:CompPipeline}. 

\section{Satellite Imaging Architectures}
Since the camera is going to be capturing pictures of the earth, it would be required to study the existing imaging architectures currently being used in satellites and how the design of the lensless camera would fit into the existing imaging architectures. We will first look into the terminology commonly used in space instrumentation. As the imager is carried along the orbit of the earth, it images a strip on the surface of the earth. The width of the strip is called the 'swath'. The direction along which the satellite moves or images is called the 'along-track' direction and the direction perpendicular to it is called the cross-track direction\cite{ImagingGeo}. Figure \ref{fig:ImagingGeo} describes these terminology and some other terms as well.  
\begin{figure}[ht]
\includegraphics[width=\textwidth]{pics/imaginggeo}
\caption{Various Imaging Terms\cite{ImagingGeo}}
\label{fig:ImagingGeo}
\end{figure}
Three major types of scanning architectures are employed in space instruments, namely:
\begin{itemize}
\item Whiskbroom Line Scanner: In this type of scanning architecture, a detector element detects it's instantaneous field of view which is projected onto a pixel element. In this scanning type the surface of the earth is scanned in lines. A scanning mirror would project a very small area of the earth onto the single pixel element. The scanning mirror would then rotate to project the next element of the line onto the next pixel. Depending on the motion of the satellite, the next line of the detector is scanned and projected on to the next line on the surface of the earth. An advantage of this type of detector is that it would be possible to obtain a very large field of view. However, it also comes with disadvantage that a very high sampling frequency is required to get decent resolutions. Typically, an earth observation satellite would move at 6.5 km per second. In order to get a resolution of 100 meters per pixel, it would be required to sample atleast 65 lines per second. For a swath of 1000 pixels it would be required to sample at 65000 elements per second. Apart from this, there is very limited time for each detector element which would result in low spatial resolution\cite{SpInst}. Another main disadvantage is that mechanical components would be required to project different parts of the surface of the earth on to the detector element. This type of scanner is also called as along-track scanner. Mathematically, the measurement of the detector element $(X,Y)$ can be described using 
$$
(X,Y) = f(t_x, t_y)
$$
where $t_x$ and $t_y$ is the time at which the image is captured in the corresponding location
%\begin{figure}[ht]
%\begin{subfigure}
%\includegraphics[width=0.5\textwidth]{pics/WhiskBroom}
%\caption{Whiskbroom Scaning Architecture}
%\label{fig:WhiskBroom}
%\end{subfigure}
%\begin{subfigure}
%\includegraphics[width=0.5\textwidth]{pics/PushBroom}
%\caption{Pushbroom Scaning Architecture}
%\label{fig:PushBroom}
%\end{subfigure}
%\end{figure}
\begin{figure}[ht]
\centering
\begin{subfigure}{0.75\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{pics/WhiskBroom}
  \caption{WhiskBroom Scanning}
  \label{fig:Whiskbroom}
\end{subfigure}
\begin{subfigure}{0.75\textwidth}
  \centering
  \includegraphics[width=.65\linewidth]{pics/PushBroom}
  \caption{Pushbroom Scanning}
  \label{fig:Pushbroom}
\end{subfigure}
\caption{A figure with two subfigures}
\label{fig:test}
\end{figure}

\item Pushbroom Line Scanner : In this type of architecture, the orbital motion of the sensor is used to image the swath instead of using a mirror as in the case of whiskbroom scanner. The field of view in the cross-track direction is imaged by the corresponding line detector array. Successive lines are imaged and sampled by the multiplexer as the sensor moves across the surface. The time between sampling two successive lines can be the time it takes for the satellite to move that distance. The most commonly used detector for a pushbroom scanner is Charge Coupled Devices(CCD). One of the main advantages of this type of scanner is that it requires no moving parts. Due to this, it is possible to obtain very high scanning rates(< $1 \mu$ second). This also leads to lower noise in the received signal\cite{SpInst}. The disadvantage is that large number of detectors are required to image a large piece of area. In addition to this, it requires an optical arrangement that could obtain a wide field of view. Mathematically, the measurement of the detector element $(X,Y)$ can be described using 
$$
(X,Y) = f(x), f(t_y)
$$
where $f(x)$ represents the sensor output and $f(t_y)$ represents the time at which the subsequent rows are imaged.  
\item Staring Array : Staring arrays use 2-d CCD/CMOS detectors to capture an entire area on the surface of the earth. These are also called as framing cameras. This provides speed-up and step-and-stare mechanism is employed wherein observations are made intermittently after a certain number of steps in the cross-track direction. The advantage is that moderate field of view optics is only required in this case\cite{SpInst}. Mathematically, the measurement of the detector element $(X,Y)$ can be described using 
$$
(X,Y) = f(x), f(y)
$$
where $f(x)$ and $f(y)$ represents the sensor output.
\end{itemize}
\section{Trade-off Analysis}
\label{sec:tradeOff}
\subsection{Camera Sensor}
The camera sensor is the core of the Delfi-PQ Imager. The performance of a camera is mainly limited by the image sensor that it uses\cite{cmos}. The camera sensor can be off two types namely, CCD(charge coupled device) or CMOS(Complimentary Metal Oxide Semiconductor). Both the types of CMOS sensors have their own advantages and disadvantages. To understand the challenges that each type of sensor poses, we must understand how the sensors are designed.

The following factors have been chosen to  make a trade-off between the different CMOS sensors:
\begin{enumerate}
\item Resolution : When rating a camera, the first thing that comes to the mind is the resolution of the camera. The resolution of a camera is directly dependent on the number of pixels in the image sensor of the camera. 
\item Power Consumption : In the design of the PQ-Camera, the most important factor is the power consumption of the entire imager. The majority of the power consumption by the imager is dependent on the power consumption of the CMOS sensor. 
\item Availability : Even though there are innumerable number of CMOS sensors in the world, availability of CMOS sensors is quite low when it comes to small-scale. Many CMOS manufacturers require large scale orders.
\item Quantum Efficiency(QE) : Quantum Efficiency is the measure of efficiency of the camera sensor to convert incoming photons into electrons. The ratio of electrons generated during the digitization process to photons is called quantum efficiency.
\item Pixel Size : Pixel size is the size of each pixel unit in the CMOS camera. It is also an important factor considering that the signal produced by the CMOS sensor depends on the pixel size as well.
$$
Signal = Light Density * (Pixel Size)^2 * QE
$$
\item Electronic Interface : The electronic interface that can be used to retreive data from the CMOS sensor also plays an important role. Since the project uses a low-power microcontroller that has limited communication capabilities, it would be wise to chose an interface that is supported by the microcontroller. Recently available chips use LVDS/MIPI interface to send data. These interfaces are not supported by the microcontroller that is being used as an on-board computer. The on-board computer uses an I2C based interface and that the electronic interface would be an important factor as it would reduce the complexity of the system and also reduce the power consumption by removing the addidional circuitry necessary for interfacing with the onboard computer.

\item Dynamic Range : Dynamic Range and SNR are used interchangeably in CMOS sensors. The only difference is that dynamic range considers only the temporal dark noise while SNR includes the root mean square of the shot noise as well.  

\item Shutter Type : Camera sensors use different types of shutters namely, global shutter and rolling shutter. Global shutter reduces the distortions due to fast moving artefacts while increasing the dark current. Rolling shutter has more distortions in the case of imaging moving artefacts, but also has lesser dark noise compared to global shutter. 

\item Voltage Level : Voltage level also has to be taken into account while choosing the sensor because if the CMOS sensor needs a voltage level higher than that of the main satellite bus voltage, then additional circuitry has to be introduced to step up the voltage level which in turn increases the overall system power.  

\item Operating Temperature : Operating temperature is an important factor to take into account when choosing an imaging sensor. Since the camera is going to operate in space, it is better if the CMOS sensor has a higher operating range of temperature. 

\item Overall Size and Weight : As the imager has to fit within specific dimensions, the overall size and weight of the CMOS sensor also needs to be taken into account.

\item Frame Rate: Even though, it is not required to have a camera sensor that is capable of high frame rates, it is an added advantage and higher frame rate camera could help in imaging larger areas of the earth if required. 
\item Price: While there are no specific cost constraints in the project, price has also been taken into account.
\end{enumerate}

In \cite{surveyCamMod}, a survey of camera modules for a CubeSat space Mission has already been carried. However, we also consider image sensors(not same as camera modules) as we are fundamentally changing the design of a camera. 

The following candidates have been chosen for analysis. These candidates are chosen based on \cite{surveyCamMod} and also on the latest CMOS sensors available on the market. 
\begin{enumerate}[label=(\alph*)]
\item IDS UI- 1646LE USB 1.3MP
\item C3188A
\item PC67XC-2 CCD
\item MicroCam TTL
\item PB-MV40
\item Omnivision OV7670
\item Sony ICX285AL
\item Omnivision OV5642
\item Omnivision OV2740
\item MCM20027
\end{enumerate}
\begin{table}[ht]
\caption{Comparison of Different Image Sensor Candidates}
\label{tbl:TradeoffCMOS}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
\hline
\diaghead{\theadfont Diag ColumnmnHead II}%
{Factors}{Candidates}&
\thead{(a)}&\thead{(b)}&\thead{(c)}&\thead{(d)}&\thead{(e)}&\thead{(f)}&\thead{(g)}&\thead{(h)}&\thead{(i)}&\thead{(j)}\\
\hline
\textbf{Optical Parameters} & & & & & & & & & &\\
\hline
Resolution & ++ & + & + & - & ++ & + & ++ & ++ & ++ & ++ \\
\hline
Pixel Size & + & ++ & X & ++ & ++ & + & ++ & + & + & ++ \\
\hline
Shutter type & - & - & X & - & - & + & + & + & + & - \\
\hline
Frame Rate & + & + & + & + & +++ & + & + & + & + & + \\
\hline
\textbf{\makecell{Electrical and \\other parameters}} & & & & & & & & & & \\
\hline
Power Consumption & - - & - -  & - - - & ++ & - & ++ & - & - - - & - -& - - \\
\hline
Availability & - & + & - - - & - - -& - - - & ++ & - - & ++& - -& - - -\\
\hline
Electronic Interface & + & + & - - - & ++ & - - & ++ & - - & ++ & - & +\\
\hline
DR and SNR & X & + & X & ++ & + & ++ & + & ++& + & +\\
\hline
Voltage & + & + & - - - & ++ & + & ++ & + & ++ & - & +\\
\hline
Operating Temperature & + & + & + & + & + & + & + & + & + & +\\
\hline
Overall Size and Weight & + & + & + & + & + & + & + & + & + & + \\
\hline
Price & - & + & X & X & - - - & ++ & X & + & X & X\\
\hline
\textbf{Points} & 2 & 8 & -8 & 8 & 0 & 18 & 5 & 13 & 2 & 4\\
\hline
\end{tabular}
\end{table}

\subsection{Compression Algorithms}
Data Compression plays a very important role when it comes to space missions. It is a very important aspect in the system design of a lensless camera for space application as the image that needs to be sent down to earth needs to be compressed as much as possible. Various surveys\cite{Compression2}\cite{Compression3}\cite{Compression4} have been conducted previously in-terms of which compression algorithm would best preserve the data and provide maximum compression at the same-time. Compression algorithms can be divided into two types namely:
\begin{itemize}
\item Lossless Compression :
\item Lossy Compression :
\end{itemize}
\subsection{Reconstruction Algorithms}
The simplest lensless imaging system is the pinhole camera. However, since the quality of the image depends on the size of the pinhole, that restricts the amount of light that can enter the imaging system. Lenses were introduced to focus the light from distant objects onto a film or a sensor. In the absense of a lens, the sensor would record the average intensity of the light entering it. This can also bee seen in the experiments which are described in the upcoming chapters. Coded aperture cameras extend the pinhole camera concept replacing the single aperture with a mask containing multiple apertures. The first developed coded aperture cameras were used in imaging X-Ray sources due to the difficulty involved in focussing light from X-Ray sources\cite{Cannon1}. Since a single pinhole limits the amount of light imaged by the sensing element, it was replaced by many holes, called the aperture so that overlapping images are formed on the film. The recorded image will have no similarity with the source and an digital processing is required to reconstruct the source image or the object. The recorded image is mathematically modelled as a  collection of overlapping shadows as described by the following equation\cite{VBoomi}\cite{Cannon1} 
\begin{equation}
y = \phi * x + e
\end{equation}

where $y$ represents the image formed on the sensor, $\phi$ represents the mask pattern, $x$ represents the irradiance vector or the object and $e$ represents the noise. The $*$ operator represents the convolution operation between the mask an the object. The coded aperture increases the flux that falls on the detector and this leads to an increase in the SNR. The SNR can be as large $\sqrt{N}$, where N represents the number of holes in the aperture\cite{Cannon1}. The increased SNR comes at the cost of computational decoding for the image. 